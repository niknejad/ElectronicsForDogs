%!TEX root = edance.tex


\chapter{Linear Time-Invariant Systems}




\graphicspath{{./figs_LTI/}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chapter Preview}


In this chapter we will review linearity in the context of an Linear Time-Invariant (LTI) system by making analogies with discrete finite dimensional systems.  Many people have familiarity with concepts such as orthogonality and eigenfunctions from linear algebra.  It's not critical to have this background, so these references can be skipped.  Most importantly, we will show that complex exponential functions are eigenfunctions of linear systems and they span the space of solutions.  We will define the time-domain impulse response and convolution operator and in a very natural way show the relation between the impulse response and the transfer function in the frequency domain.  This will lead us in to the a preview of the frequency domain representation of linear systems, which we'll pick up in the next chapter.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Example:  Analyzing an $RLC$ Circuit}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Example:  Low Pass Filter (LPF)}

Let's begin with an example calculation.  Suppose we have a low pass filter as shown in Fig.~\ref{fig:lpf}.  If we drive this system with a source $v_s$ and observe the output at $v_o$, we can write the governing equations as follows by using KVL around the loop:
\be v_o(t) = v_s(t) - i(t) R \ee
Since
\be i(t) = C \frac{dv_o(t)}{dt}  \ee
we have
\be v_o(t) = v_s(t) - RC \frac{dv_o(t)}{dt}  \ee
or in terms of the circuit time constant $\tau = RC$:
\be v_o(t) = v_s(t) - \tau \frac{dv_o(t)}{dt}  \ee

Let's suppose that the input signal is given by $v_s(t) = V_s \cos(\omega t)$.  We know that in steady-state  the output amplitude and phase will change: 

\be v_o(t) = \underbrace{K \cdot V_s}_{V_o} \cos(\omega t + \phi) \label{eq:kvl1} \ee
  
How do we find the change the amplitude and phase of the signal?

\begin{figure}[tb]
\begin{center}
\includegraphics[scale=1]{lpf}
\end{center}
\caption{A simple $RC$ low-pass filter. } \label{fig:lpf}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{LPF the ``hard way"}


This is the wrong way to solve the problem.  It's important to review it to see how painful it can be even for a simple example.  We start by plugging the known form of the output into the governing equation(Eq.~\ref{eq:kvl1} and verify that it satisfies the equation:
\be
V_s \cos(\omega t) = V_o \cos(\omega t + \phi) - \tau \omega V_o \sin(\omega t + \phi)
\ee
Use the following identities:
\be
   \cos(x + y) = \cos x \cos y - \sin x \sin y
\ee
\be
   \sin(x + y) = \sin x \cos y + \cos x \sin y
\ee
We have
\be
V_s \cos(\omega t) = V_o \cos(\omega t) (\cos \phi - \tau \omega \sin\phi) -
V_o \sin(\omega t) (\sin\phi + \tau \omega \cos\phi)
\ee
Since sine and cosine are linearly independent functions:
\be
          a_1 \sin(\omega t) + a_2 \cos(\omega t) = 0
\ee
This implies that $a_1 = 0$ and $a_2 = 0$ must be zero independently.  Applying the linear independence gives us
\be
	-V_o \sin\phi - V_o \tau \omega \cos \phi = 0
\ee
and
\be
	\tan \phi = -\tau \omega
\ee
 The phase response is therefore 
\be
	\phi = - \tan^{-1} \tau \omega
\ee
Likewise we have
\be
	V_o \cos\phi - V_o \tau \omega \sin\phi - V_s = 0
\ee
\be
	V_o (\cos \phi - \tau \omega \sin\phi) = V_s 
\ee
\be
	V_o \cos \phi (1  - \tau \omega \tan\phi) = V_s 
\ee
\be
	V_o \cos \phi (1  + (\tau \omega)^2 ) = V_s 
\ee
\be
	V_o (1  + (\tau \omega)^2 )^{1/2} = V_s 
\ee
 The amplitude response is therefore given by
\be
 \frac{V_o}{V_s} = \frac{1}{\sqrt{1 + (\tau \omega)^2}}
\ee
We can see that both the amplitude and phase response are a strong function of frequency.  At very low frequencies, $\omega \approx 0$, the signal passes undisturbed.  At very high frequencies $\omega \rightarrow \infty$, the signal experiences infinite attenuation and is effectively shorted out by the capacitor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{LPF Magnitude and Phase Response}

Plots of the magnitude and phase response are shown in Fig.~\ref{fig:magphase}.  The magnitude plot shows that for signal frequencies below the cutoff frequency, $1/\tau$, the signal experiences little attenuation.  In fact, at the passband edge frequency of $1/\tau$, the magnitude is by definition lower by $1/\sqrt{2}$, which corresponds to half the power.  Likewise, the phase response is zero at low frequencies, meaning the signal is not delayed, but even at the cutoff frequency the delay is $45^\circ$, and asymptotically reaches $90^\circ$.

As is common practice in electrical engineering, we more often plot the magnitude on a log-log scale with units of dB,  as shown in Fig.~\ref{fig:magphasedB}.  On the deciBel (deci = 10) scale, we take the base 10 $\log_{10}$ and multiply by $10$ to get rid of fractional parts.  The "Bel" part is in honor of the inventor of the phone,.  The dB scale allows us to expand the passband and more clearly understand the behavior of the transfer function as a function of frequency.  The multiplication by 10 is just for convenience.

\begin{figure}[tbp]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=.6\columnwidth]{mag} \\
(a) \\
\includegraphics[width=.6\columnwidth]{phase} \\
\end{tabular}
\end{center}
\caption{The (a) magnitude and (b) phase response of the low-pass filter. } \label{fig:magphase}
\end{figure}

\begin{figure}[tbp]
\begin{center}
\begin{tabular}{c}
(b) \\
\includegraphics[width=.6\columnwidth]{dB} \\
(c) \\
\end{tabular}
\end{center}
\caption{The log-log plot of the magnitude response of a low-pass filter (dB scale) . } \label{fig:magphasedB}
\end{figure}


But you'll notice that we often multiply log by $20$ rather than $10$ in the definition of $dB$?   Power is proportional to voltage squared:
\be
 \mathrm{dB} = 10 \log\left( \frac{V_o}{V_s} \right)^2 = 20 \log\left( \frac{V_o}{V_s} \right)
\ee
which means that if we take the $\log$ of voltage, we can just multiply by 2 o convert it into power.

At various frequencies we have:

\be \omega = 1/\tau \rightarrow \left( \frac{V_o}{V_s} \right)_{\mathrm{dB}} = -3 \mathrm{dB} \ee

\be \omega = 100/\tau \rightarrow \left( \frac{V_o}{V_s} \right)_{\mathrm{dB}} = -40 \mathrm{dB} \ee

\be \omega = 1000/\tau \rightarrow \left( \frac{V_o}{V_s} \right)_{\mathrm{dB}} = -60 \mathrm{dB} \ee

Observe that the slope of signal attenuation is 20 dB/decade in frequency beyond the passband.   Alternatively, if you double the frequency, the attenuation changes by 6 dB, or 6 dB/octave.  This is an important rule of thumb when understanding the frequency response of a system.


\section{Linear Time-Invariant Systems}

\subsection{Linear Time-Invariant Definition}

A system $\mathbf{L}$ is linear if it satisfies the following requirement:
\be
	\mathbf{L} [ \alpha x(t) + \beta y(t) ] = \alpha \mathbf{L}[ x(t) ] + \beta \mathbf{L}[ y(t) ] 
\ee
In this notation, $\mathbf{L}$ is an operator that corresponds to the action of the system at hand. If we input $x(t)$ an input into our system, the system outputs $y(t)$.  In a linear system, if we scale the input, we also scale the output by the same amount. Also, if we take a superposition of inputs, the output is the same superposition of the inputs.

Now a system is time invariant if its response is not a function of time. In other words, the system does not "age" and it does not care when you apply the input.  If you apply an input today and observe the output, and come back tomorrow and put in the same input, the output should look the same.  Mathematically, we can summarize this behavior with the following equations:
\be
	y(t) = \mathbf{L} [  x(t)  ] 
\ee
\be
	 \mathbf{L} [  x(t-\tau)  ] = y(t-\tau) 
\ee
Notice that time shifting the input just corresponds to applying the same time shift to the output.  Another way to think of time-invariance is to say that the system has no  ``clock" or time reference, or that the transfer function is not a function of time.	 It does not matter when you apply the input.  The transfer function is going to be the same. 

 If you are familiar with discrete linear system, you know that lineary and superposition are fundamental properties. 

\subsubsection{Vectors and Matrices Become Functions and Integrals}

 In finite dimensional discrete linear systems, inputs and outputs are defined in terms of input and output vectors.  For LTI systems, we can think of the inputs and outputs as infinite dimensional vectors, but instead of a countably infinite set of elements we have a uncountably large number of time instants that define the input and output.  In other words, we reprsent the input and output as continuous fucntions of time.   We will see that in LTI systems, we can also represent linear systems by basis functions:
\be
	\phi_1(t), \phi_2(t), \cdots 
\ee
and instead of a matrix representation (summation) we will have an integral representation called the Convolution Integral.  Eigenvectors have the direct analog of Eigenfunctions:
\be
	\mathbf{L} [  \phi_n(t)  ] = \lambda_n  \phi_n(t) 
\ee
And the concept of an Orthonormal Basis maps directly into LTI systems:
\be
	x(t) = \sum_n \phi_n(t) x_n
\ee
\be
	x(t) = \int \phi(s t) x(t) dt
\ee

We can therefore perform Eigenfunction expansion of an arbitrary input and study the response of a linear system to an arbitray input by using our knowledge of how the system responds to the eigenfunctions, in other words the eigenvalues of the system, or the spectral expansion, will be the most important aspect of a linear system.

If this doesn't make any sense, don't worry.  We'll cover all of this in a step-by-step fashion. This is only meant as a teaser for people who have seen linear system theory previously. 


\section{The Complex Exponential Technique}

We begin by introducing the complex expontial, and we will show that complex exponentials are eigenfunctions of linear systems.  This means that they play an immensely central role in linear system analysis.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Why introduce complex numbers?}


Complex exponentials actually make things less complex.  To see this, first note that integration and differentiation are trivial with complex exponentials:
\be \frac{d}{dt} e^{j\omega t} = j \omega e^{j\omega t} \ee
\be \int e^{j\omega x} dx = \frac{1}{j \omega} e^{j\omega t} \ee

So now any ordinary differential equation (ODE), a sum of differentiation operators, is reduced now to trivial algebraic manipulations.  In fact, we'll show that you don't even need to directly derive the ODE by using phasors (phasor is essentially a shorthand notation for a complex number).   The key is to observe that the current/voltage relation for any element can be derived for complex exponential excitation.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Complex Exponential}


From Eulor's we have an important relationship between complex exponentials and ordinary $\sin$ and $\cos$ functions:
\be  e^{j x} = \cos x + j \sin x \ee
If take the magnitude of this quantity, it's unity
\be | e^{j x} | = \sqrt{\cos^2 x +  \sin^2 x}  = 1 \ee
This means that $e^{j\phi}$ is a point on the unit circle at an angle of $\phi$ from the $x$-axis. 

Any complex number $z$ (Fig.~\ref{fig:complex}), expressed as have a real and imaginary part $ z = x + j y$, can also be interpreted as having a magnitude and a phase.  The magnitude  $|z| = \sqrt{x^2 + y^2}$ and the phase $\phi = \angle z = \tan^{-1} y/x$ can be combined using the complex exponential
\be
     x + j y = |z| e^{j\phi}
\ee

\begin{figure}[tb]
\begin{center}
\includegraphics[width=.6\columnwidth]{complex}
\end{center}
\caption{A general complex number $z$ can be repesented by its real and imaginary components or its magnitude and phase. } \label{fig:complex}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Eulor's Relations and The Circle}


The argument to $e^{j \omega t}$ is just a linearly increasing with a slope of $\omega$, and completes $2\pi$ radian every $\frac{2\pi}{\omega}$ seconds.  This implies that $e^{j \omega t}$ is nothing but a point rotating on a circle on the complex plane.  The real part and imaginary parts are just projections of the circle, which by trigonometry we know equal the cosine and sine functions.

 We can also express $\cos$ and $\sin$ in terms of $e$ as follows
\be
         \cos x = \frac{e^{jx} + e^{-jx}}{2}
\ee
\be     \sin x = \frac{e^{jx} - e^{-jx}}{2j} 
\ee
which shows that two counter rotating complex exponentials sum to $\sin$ and $\cos$.  We can visualize these relations as shown in Fig.~\ref{fig:complexexp}.  Be sure to click the links to see an animation, especially for the sum of two counter rotating complex exponentials in Fig.~\ref{fig:cos}.


\begin{figure}[tb]
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=.4\columnwidth]{exp1.pdf}  &
\includegraphics[width=.4\columnwidth]{exp2.pdf} \\
(a) & (b) \\
\end{tabular}
\end{center}
\caption{  (a) The complex exponentail $e^{j\omega t}$ rotates around the unit circle counter-clockwise at a rate of $\omega$, or once every $2\pi/\omega$ seconds.  Likewise (b) $e^{-j\omega t}$ rotates around the unit circle clockwise.  See \href{rfic.eecs.berkeley.edu/~niknejad/photos/ee105/exp1.mov}{$e^{j\omega t}$ animation} and \href{http://rfic.eecs.berkeley.edu/~niknejad/photos/ee105/exp2.mov}{$e^{-j\omega t} animation$}.
}
  \label{fig:complexexp}
\end{figure}


\begin{figure}[tb]
\begin{center}
\includegraphics[width=.5\columnwidth]{exp3.pdf}
\end{center}
\caption{We visualize the function $\cos$ by building it through vectorwise addition of $e^{j\omega t}$ and $e^{-j\omega t}$.  See the \href{http://rfic.eecs.berkeley.edu/~niknejad/photos/ee105/exp3.mov}{animation}.
}
\label{fig:cos}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Magic of Sinusoids}

\begin{figure}[tb]
\begin{center}
\includegraphics[width=.75\columnwidth]{lti_sine}
\end{center}
\caption{In an LTI system, if we drive it with a sinusoid, the output is a sinusoid at the sam frequency, with a new amplitude and phase. } \label{fig:lti_sine}
\end{figure}

What were going to show is that when a linear, time invariant (LTI) circuit is excited by a sinusoid (see Fig.~\ref{fig:lti_sine}), it's output is a sinusoid at the \emph{same} frequency.  Only the magnitude and phase of the output differ from the input.  Sinusoids are very special functions for LTI systems, in other words they are eigenfunctions.  The ``Frequency Response'' is a characterization of the input-output response for sinusoidal inputs at all frequencies.

Since most periodic (non-periodic) signals can be decomposed into a summation (integration) of sinusoids via Fourier Series (Transform), the response of a LTI system to virtually any input is characterized by the frequency response of the system.  We'll return to this point later.

\subsubsection{Complex Exponential is Powerful}

To find steady state response we can excite the system with a complex exponential.   Here are system is any general LTI system. In terms of circuits, it's any linear circuit consisting of resistors, capacitors, inductors, mutual inductors, transformers, and linear dependent soruces.  At any frequency, the system response is characterized by a single complex number $H$, as shown in Fig.~\ref{fig:lti_resp}.   The magnitude response is given by $|H(\omega)|$ and
the phase response is given by $\angle H$.   We see that the complex exponential is an ``eigenfunction" of the system.  It is used to probe the system.  If we characterize the response to all eigenfunctions, we can completely characterize the system.   Because a sinusoid is a sum of complex exponentials (and because of linearity!), we can also probe a system by applying a real sinusoidal input.  This is what we do in the lab.


\begin{figure}[tb]
\begin{center}
\includegraphics[width=.65\columnwidth]{lti_resp}
\end{center}
\caption{The complex exponential response of a linear system is another complex exponential, with an additional magnitude and phase factor. } \label{fig:lti_resp}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{$H$ is the Transfer Function}




In summary, $e^{j\omega}$ is an eigenfunction for any linear system (circuit), and $H(\omega)$ is the eigenvalue of the system.  There's a continuous spectrum of eigenvalues $H(\omega)$.   The linear system is completely characterized by the spectrum of eigenvalues, or in the frequency domain by the transfer function $H(\omega)$.  We also often write $H(\omega)$ in the following form (explained shortly):
\be
	H(j\omega)
\ee

 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Low Pass Filter Again}




\subsection{LPF Example:  The ``soft way''}

Remember that earlier we tried to analyze the low-pass filter using sinusoids.  Now let's excite the system with a complex exponential.  Recall that the sinusoidal response is related to the exponential response so we can always recover the sinusoidal response from the complex exponential response
\be
	 v_s(t) = v_o(t) + \tau \frac{dv_o}{dt} 
\ee
Derivatives are simply products with $j\omega$
\be 
	v_s(t) = V_s e^{j\omega t} 
\ee
Grouping terms
\be
	 v_o(t) = |V_o| e^{j (\omega t + \phi)} = V_o e^{j\omega t} 
 \ee
Now substitute into the original equation
\be 
	V_s e^{j \omega t} = V_o e^{j \omega t}  + \tau \cdot j\omega \cdot V_o e^{j \omega t} 
\ee
and divide out the non-zero common factors ($|e^{j\omega t} | = 1$)
\be 
	V_s = V_o ( 1 + j \omega \cdot \tau) 
\ee
So with a few lines of algebra, we have the transfer function:
\be \frac{V_o}{V_s} = \frac{1}{1 + j\omega \cdot \tau}\ee

 The system is characterized by the complex function
\be 
	H(\omega) = \frac{V_o}{V_s} = \frac{1}{1 + j\omega \cdot \tau} 
\ee
 The magnitude and phase response match our previous calculation:
\be 
	| H(\omega) | = \left|\frac{V_o}{V_s}\right| = \frac{1}{\sqrt{1 + (\omega \tau)^2}}
\ee
\be 
	\angle H(\omega) = - \tan^{-1} \omega \tau 
\ee
Clearly, using the complex exponential is much faster and easier than the earlier approach where we used sinusoids directly and the final answer is the same.  It's not obvious why the transfer function for a complex exponential matches the one for the sinusoidal excitation.  Let's explore this further.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Why did it work?}

If we push the complex exponential through the system and take the real part of the output, then why is that the same as the ``real'' sinusoidal response?  One argument to explain why the complex exponential works in place of a sinusoid is to observe that sine and cosine are simply imaginary and real parts of the complex exponential.  So any signal (current or voltage) can be written in the following equivalent forms
\be 
	s(t) = S_o \cos(\omega t + \phi) = S_o \Re[e^{j(\omega t + \phi)}]
\ee
For example, if we excite our system with an input $z(t)$ that is complex, producting an output $y_z(t)$, and if we take the real or imaginary part of the signals, we have
\be
	 y_z(t) = \mathbf{L}(z(t))
\ee
Now take the real part of both sides and use the fact that the order of the operator $\Re$ and our system $mathbf{L}$ can be interchanged:
\be 
	\Re[y_z(t)] = \Re[\mathbf{L}(z(t))]  = \mathbf{L} (\Re[z(t)]) 
\ee
If this argument does not convince you, there's a slightly more complicated way to show this must be true.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
\begin{center}
\includegraphics[width=.75\columnwidth]{lti_cosine}
\end{center}
\caption{Sinusoidal response of an LTI system by superposition. } \label{fig:lti_cosine}
\end{figure}


\subsubsection{And yet another perspective}

As shown in Fig.~\ref{fig:lti_cosine}, another way to see this is to observe the system is linear so that we can push through two inputs separately and sum the outputs:
\be
          y = \mathbf{L}(s_1(t) + s_2(t)) = \mathbf{L}(s_1(t)) + \mathbf{L}(s_2(t)) 
\ee
Similarly, to find the response to a sinusoid, we can find the response to $e^{j\omega t}$      and  $e^{-j\omega t}$  and sum the results.  For a linear system represented by real circuit elements, if  the input is real function of time, the output must also be a real function:
\be 
	y(t) = \frac{H(\omega) e^{j\omega t} + H(-\omega) e^{-j\omega t} }{2}
\ee
The only way for two complex numbers to sum to a real number is if they are complex conjugates.  That means the second term is the conjugate of the first, which implies that
\be
	|H(-\omega)| = |H(\omega) |
\ee 
Or in other words, the magnitude response is an even function of frequency.  Likewise, we have
\be
	\angle H(-\omega) = -\angle H(\omega) = - \phi 
\ee 
Or the phase function has to be an odd function of frequency.  Therefore the output is given by
\be
	 y(t) = \frac{|H(\omega)|}{2} \left( e^{j(\omega t + \phi)} + e^{-j(\omega t + \phi)} \right) 
\ee
or more simply
\be = |H(\omega) | \cos(\omega t + \phi) \ee




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Generalization to any Linear Circuit}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{``Proof'' for Linear Systems}

%\insfig{}{.85}

 For an arbitrary linear circuit, in other words a black box containing an arbitrary number of inductors, capacitors, resistors, mutual inductors, or linear dependent sources, we can  decompose the system into linear sub-operators, like multiplication by constants, time derivatives, or integrals:
\be 
y = \mathbf{L}(x) = a x + b_1 \frac{d}{dt} x + b_2\frac{d^2}{dt^2} x + \cdots + c_1 \int x + c_2 \iint x + \cdots
\ee 
For a complex exponential input this simplifies to:
\be 
y  = \mathbf{L}(e^{j\omega t}) = a e^{j\omega t} + b_1 \frac{d}{dt} e^{j\omega t} + b_2\frac{d^2}{dt^2} e^{j\omega t} + \cdots + 
    c_1 \int e^{j\omega t} + c_2 \iint e^{j\omega t} +  \cdots 
\ee
\be
y  = a e^{j\omega t} + b_1 j\omega e^{j\omega t} + b_2 ( j\omega)^2 e^{j\omega t} + \cdots + c_1 \frac{e^{j\omega t}}{ j\omega} + c_2 \frac{e^{j\omega t}}{ (j\omega)^2} + \cdots
\ee 
Note that every term is of the form $e^{j\omega t}$ times a constant, which when grouped together gives
\be
y =  e^{j\omega t} \underbrace{\left(a  + b_1 j\omega  + b_2 ( j\omega)^2  + \cdots + c_1 \frac{1}{ j\omega} + c_2 \frac{1}{ (j\omega)^2}  + \cdots \right)}_{H}
\ee
We've grouped together all the terms multiplying $e^{j\omega t}$ and note that at a given frequency, they sum to a complex number $H$.    The amplitude of the output is the magnitude of the complex number $H$ and the phase of the output is the phase of the complex number $H$.
\be
y =  e^{j\omega t} \underbrace{\left(a  + b_1 j\omega  + b_2 ( j\omega)^2  + \cdots + \frac{1}{ j\omega} + \frac{1}{ (j\omega)^2}  + \cdots \right)}_{H}
\ee
Written more comptactly, we have
\be
 y = e^{j\omega t} |H(\omega)| e^{j\angle H(\omega)}
 \ee
 or equivalently
\be
	\Re[ y ] =  |H(\omega)| \cos ( \omega t + \angle H(\omega))
\ee
 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{General Complex Exponential}


Looking back at our derivation, it's clear that a complex exponential is also an eigenfunction of our system, and the periodic complex exponential is just a special case.  We can make the same argument for a general input of the form $e^{st}$:
\be 
y = \mathbf{L}(x) = a x + b_1 \frac{d}{dt} x + b_2\frac{d^2}{dt^2} x + \cdots + c_1 \int x + c_2 \iint x + \cdots
\ee 
\be
y =  e^{s t} \underbrace{\left(a  + b_1 s  + b_2 s^2  + \cdots + \frac{1}{ s} + \frac{1}{ s^2} +  \cdots \right)}_{H(s)}
\ee
Later we'll see that $H(s)$ can be used to solve a problem involving the transient response of a system (response to an initial input) whereas $H(j\omega)$ is used to find the steady-state response to a sinusoidal input.
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Complex Plane}

In Fig.~\ref{fig:splane} we show the ``s-plane", and the x mark particular complex values of $s$.  Notice that the real axis corresponds to the exponential envelope growth or decay.  The imaginary axis corresponds to the rate of oscillation.  On the real axis, there's no oscillation at all. On the imaginary axis, there's no decay.  This plot shows that the general exponential $e^{st}$ can represent different kinds of functional behavior including steady-state oscillation (sinusoidal response) and other decaying and growing oscillation amplitudes.

\begin{figure}[tb]
\begin{center}
\includegraphics[width=.85\columnwidth]{exp_graph.pdf}
\end{center}
\caption{On the complex plane, the location of the argument $s$ to $e^{st}$ corresponds to either sinusoidal harmonic oscillation (on the imaginary axis), or exponential growth or decay (on the real axis).  Other off-axis locations are a combination of decaying or growing exponential sinusoidal waveforms.} \label{fig:splane}
\end{figure}



 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Time Domain Characterization of Linear Systems}





\subsection{Unit Area Rectangular Function}


\begin{figure}[tb]
\begin{center}
\includegraphics[width=1\columnwidth]{rect_func}
\end{center}
\caption{The unit rectangular function, centered at the origin or shifted by $\tau$. } \label{fig:fun_rect}
\end{figure}


A very convenient function, the unit rectangular function, shown in Fig.~\ref{fig:fun_rect}, is defined as follows.  Consider a rectangular function with unit area:
\be
	r(t) = \left\{ 
		\begin{array}{cc} 
			\frac{1}{\Delta} & -\Delta/2 < t < \Delta/2 \\ 
			0  & \mathrm{otherwise} 
		\end{array} \right.
\ee
 By definition, we have a unit area function
\be
	\int_{-\infty}^{\infty} r(t) dt = 1
\ee
 

\subsubsection{Function Approximation}

We can approximate any function as a sum of rectangular shifted functions
\be
	x(t) \approx \sum_i x(i\tau) r(t - i\tau) \Delta
\ee
 As the rectangles become smaller and smaller, the representation becomes more accurate
 The key question is what happens as we take the limit of a very small time step $\Delta$?
 


\begin{figure}[tb]
\begin{center}
\includegraphics[width=.55\columnwidth]{func_rect}
\end{center}
\caption{Representation of a general function by a sum of shifted rectangular functions. } \label{fig:func_rect}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{The Delta Function $\delta(t)$ -- A Strange Beast}




\begin{figure}[tb]
\begin{center}
\includegraphics[width=.5\columnwidth]{rect_func_delta}
\end{center}
\caption{The "Delta Function" can be obtained through a limiting process by making the interval of the rectangular function shorter and shorter. } \label{fig:rect_func_delta}
\end{figure}


Now imagine taking the limit as $\Delta\rightarrow 0$
\be
	\lim_{\Delta\rightarrow0}{r\left(t\right)=\delta(t)}
\ee
What do we get?  Is this thing even a function?  A pictorial representation of the limiting process is shown in Fig.~\ref{fig:rect_func_delta}.  We see that the function becomes more and more narrow and taller and taller.

 No, it's not an ordinary function.\footnote{Mathematicians have made sense of it.  Now it's known as a distribution or generalized function.}  The delta function was originally introduced by Oliver Heaviside $\sim$1900, Dirac co-invented it and gets credit as it's known as the ``Dirac Delta Function".\footnote{Heaviside was derided by the mathematicians of the day for his methods.  He often got the right answer without beging mathematically rigorous, but that's because he way ahead of everyone in his techniques and it took a while for the mathematical world to catch up.}
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Dirac Delta Sifting Property}




\begin{figure}[tb]
\begin{center}
\includegraphics[width=.5\columnwidth]{delta_sift}
\end{center}
\caption{The "sifting" property of the Delta function yields the function value at any point. } \label{fig:delta_sift}
\end{figure}



As shown in Fig.~\ref{fig:delta_sift}, one interesting thing about the Delta function is that it can be used to basically pick out a certain value of a function (because it's zero except at one location):
\be
	x\left(\tau\right)=\int\delta\left(t-\tau\right)x\left(t\right)dt
\ee
This is not obvious unless you take the limit and then it becomes clear that the integral converges to the value of the function.  So the Dirac Delta function is better defined in terms of this property, and many functions satisfy this function and can be used to define the Delta function.

Recall that we started by using the rectangular function to approximate any arbrary function.  As we take the limit, we are reconstructing our function point-by-point using weighted sums of Delta functions.  To see this, simply start with the approximated  function as before
\be
	x(t) \approx \sum_i x(i\tau) r(t - i\tau) \Delta
\ee
As we take the limit, we expect that our approximation becomes more exact and we have the following integral:
\be
	x\left(t\right)=\int x\left(\tau\right)\delta\left(t-\tau\right)d\tau
\ee
In some ways this is not really a deep result in the sense of  saying that a function is a continuous stream of real numbers, and any particular value can be "sifted" out by multiplying by a delta and integrating around the delta.  If you think of the function as a vector, the delta function is like the vector $e_n = [0 \cdots 0\, 1\, 0 \cdots 0 ]^T$ with the $1$ in the n'th position. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Back to Time Domain:  Impulse Response}

Up to now we studied the sinusoidal steady-state response using the complex exponential.  But we are now armed with the right tools to tackle the general response of a linear system to an arbitrary input in time.  Using the fundamental properties of linearity and superposition, we have
\be 
	y\left(t\right)=\ \mathbf{L}\left[x\left(t\right)\right]=\mathbf{L}\left[\int\delta\left(t-\tau\right)x\left(\tau\right)d\tau\right]
\ee
\be
	y\left(t\right)=\ \mathbf{L}\left[x\left(t\right)\right]=\int \mathbf{L}\left[\delta\left(t-\tau\right)x\left(\tau\right)\right]d\tau= \int x\left(\tau\right)\mathbf{L}\left[\delta\left(t-\tau\right)\right]d\tau
\ee
Let's define the impulse function as the response of a linear system to a Delta function applied at time $\tau$
\be
	h(t,\tau) \equiv  \mathbf{L}[\delta(t-\tau)]
\ee
Notice that in a time-invariant system, it should not matter when we apply the Delta function, as long as we shift the output.  So if $h(t)$ is the response to a Delta function at time 0, $h(t-\tau)$ is the response to an input applied at time $\tau$.  So in summary, for a linear time-invariant system (LTI), we only need to characterize the system response to a Delta function once and for all to find
\be
	h(t) \equiv  \mathbf{L}[\delta(t)]
\ee
This means that $h(t)$ contains all the information about the linear system. If we call the Dirac Delta function an "impulse", then for obvious reasons, we call $h(t)$ the ``impulse response function". 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Convolution Operation}


We have demonstrated that any linear system input/output response can be represented by the following integral
\be 
	y(t) = \int_{-\infty}^{\infty} h(t-\tau) x(\tau) d\tau 
\ee
Think of this as the sum of the input weighted by the impulse response function.  The operation is known as the convolution operation and symbollically defined by the operator "$*$":  
\be
	y(t) = h(t) * x(t)
\ee
 The symbol $*$ is the convolution operator. You can also change variables $x = t - \tau$ and write it as
\be 
	y(t) = \int_{\infty}^{-\infty} h(x) x(t-x) (-dx) = \int_{-\infty}^{\infty} h(x) x(t-x) dx 
\ee
In this equation, $x$ is just a dummy variable.  We can substitute $\tau$ to make the equation look the same as before
\be 
	y(t) =  \int_{-\infty}^{\infty} x(t-\tau) h(\tau)  d\tau 
\ee
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Convolution Visualization}

\begin{figure}[tb]
\begin{center}
\includegraphics[width=\columnwidth]{conv_picture.png} 
\end{center}
\caption{The convolution operation visualized.  See \href{https://en.wikipedia.org/wiki/Convolution}{Wikipedia} for full details and an animation. } \label{fig:conv_visual}
\end{figure}


It's useful to visualize the convolution operation.  Note that the convolution operation takes in two functions and produces a new function.  The operation is described nicely on Wikipedia reproduced here in Fig.~\ref{fig:conv_visual}.  As described in the figure, we first express each function in terms of the dummy variable (of integration) $\tau$.  Next, we reflect one of the functions about the time axis, for example $h(\tau)$ becomes $h(-\tau)$.  The physical explanation for this will be described shortly.  Next we compute a sliding weighted-sum of the function $x(\tau)$ with $h(t-\tau)$.  Only points where the two functions intersect contribute significantly to the waveform.  







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Memory}

\begin{figure}[tb]
\begin{tabular}{cc}
\includegraphics[width=.5\columnwidth]{impulse1_long.pdf} &
\includegraphics[width=.5\columnwidth]{impulse2_short.pdf} \\
\end{tabular}
\caption{Two hypothetical impulse response functions are shown.  One is long and oscillates, the second is short and delays exponentially.} \label{fig:memory}
\end{figure}

For example, if $h(t)$ is mostly zero except over a small interval, 

The fact that the impulse response function has time duration is an indication that the system as memory.  The current output depends not only on the current input, but also past values of the input.  The longer the duration of the impulse response function, the more ``memory" the system has.  Capacitors and inductors store energy and act as memory in circuits.  They store energy in the magnetic and electric fields and resonance can occur because these forms of energy can be exchanged.  The impulse response function for a system with no memory is just another delta function !  Two examples are shown in Fig.~\ref{fig:memory} to illustrate a system with a longer vacillating impulse response versus a system that has a short and exponentially decaying impulse response.  

This should help you understand why we needed to flip the impulse response in the convolution operation (or equivalently the input function).  Notice first that the impulse function is necessarily zero until time zero. This is because the system is causal and there can be no output if there's no input.  Next, notice that by flipping the impulse response and taking a sliding weighted sum with the input, we're including the contribution of past inputs on the current input.  If the impulse response is short and decaying exponentially, then only a small amount of past inputs influence the current input.  On the other hand, if the impulse response is longer, then the system remembers the history of the input for a longer duration and we must include those terms to figure out the current output.

 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{The Delta Function ``Hammer":  Mechanical Analogy}

\begin{figure}[tb]
\begin{center}
\includegraphics[width=.5\columnwidth]{hammer_resonance}
\end{center}
\caption{A measurement setup to record the impulse response of a mechanical system. } \label{fig:hammer_setup}
\end{figure}


Another way to look at a delta function is to think of it like hitting your system with a hammer (see Fig.~\ref{fig:hammer_setup}.  It's an impulsive input that excites the system.  After such an excitation, the system will ``vibrate" in its natural modes and these vibrations will eventually die out.  The time that the vibrations last is related to the depth of the system's memory.  The vibrations are related to the fact that the system can store kinetic and potential energy and these modes complement each other.  Loss in the system causes this stored energy to eventually dissipate.  In fact, one way to measure the impulse response is exactly with the setup shown.  The hammer is implemented an electromechanical solenoidal hammer, and the actuation signal is fed to the trigger of the oscilloscope (or other recording device).  The vibrations are picked up by a force transducer and carefully recorded, yielding the impulse response.     

 

 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Step and Pulse Response}



\begin{figure}[tb]
\begin{center}
\includegraphics[width=.6\columnwidth]{impulse_resp}
\end{center}
\caption{The impulse response for the low-pass filtered analyzed earlier in the frequency domain. } \label{fig:impulse_resp}
\end{figure}


Consider a system with an impulse response shown in Fig.~\ref{fig:impulse_resp}.  This is in fact the impulse response function for a low-pass filter:\footnote{See Table of transforms later in this lecture.}
\be
	h(t) = u(t) \frac{1}{\tau} e^{-t/\tau}
\ee
Notice that the system has a time constant $\tau = RC$ that corresponds to the amount of time the system responds to an impulse input ($\tau = 1/3$).  Here $u(t)$ is the step function (zero until time zero, and then unity thereafter).  As discussed earlier, the impulse response is necessarily zero until time zero, and the $u(t)$ enforces this condition.  If you now imagine an arbitrary input, each time point is a weighted sum of the current input and past inputs, with an exponentially decaying amplitude for past inputs.  Only the recent $\sim \tau$ inputs really make an impact, and earlier inputs are ``forgotten".  
 


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[tb]
\begin{center}
\includegraphics[width=.5\columnwidth]{step_resp}
\end{center}
\caption{The step response of a low-pass filter. } \label{fig:step_resp}
\end{figure}



\subsection{Step Response}


A very interesting input is the "step input", or application of $u(t)$ to our system.  From what we know, we can calculate the response to a step by using a convolution:
\be
	y(t) = h(t) * u(t) = \left( 1 - e^{-3 t} \right) u(t) 
\ee
The response is plotted in Fig.~\ref{fig:step_resp}. The step has a very sharp transition at the input, but at the output it's smoothed out because each output point is an average of the current input and past inputs.  Since at the moment of the transition, all past inputs are zero, the output cannot track the input instantaneously.  
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Sinusoidal Response}


\begin{figure}[tb]
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=.5\columnwidth]{slow_sine.pdf} &
\includegraphics[width=.5\columnwidth]{fast_sine.pdf} \\
\end{tabular}
\end{center}
\caption{The response of a linear system to a "slow" and "fast" sinusoid.  Slow means the period of oscillation is smaller than the filter characteristic time constant $\tau$.  Fast means the opposite. } \label{fig:fast_slow_resp}
\end{figure}



As shown in Fig.~\ref{fig:fast_slow_resp}, if the input changes slower than the time scale of $\tau$, or if the frequency of the input is lower than roughly $1/\tau$, then the output is a smoothed version of the input (each time point is blurred by the width of the impulse response).  Fast transitions are smoothed out and don't appear at the output.   If the input changes very quickly, several cycles of the input are averaged out and produce only a small output.  This is how the filter works as high frequency signals are attenuated by the low-pass filter.  If the oscillation frequency is very slow with respect to $\tau$, the impulse response "looks like" a delta function with no memory and just passes the current input to the output.
 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Pulse Response}


\begin{figure}[tb]
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=.4\columnwidth]{long_pulse.pdf} &
\includegraphics[width=.4\columnwidth]{fast_pulse.pdf} \\
\end{tabular}
\end{center}
\caption{The pulse response of a low-pass filter.  Note that the shorter pulse has been expanded for clarity (see the time scale). } \label{fig:pulse_resp}
\end{figure}


Now let's see what happens to a pulse.  By superposition, this is just two unit-step responses, so we can easily reconstruct the response as shown in Fig.~\ref{fig:pulse_resp}.  If the pulse is wide, then the output has enough time to settle and the output pulse is a faithful representation of the input.  If the pulse width is shorter than the time-constant of the filter, there's insufficient time for the output to settle, and the pulse is distorted.  In high speed communication systems, we need to ensure the system has enough bandwidth to allow the highest frequency pulses to get through.  
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Frequency Domain Characterization of Linear Systems}

We started this chapter by discussion of the steady-state sinusoidal response of a linear system.  We found that we can use complex exponential inputs to quickly and efficiently calculate the output, and by sweeping the frequency, we obtain the spectral response of the system $H(s)$ (the complex eigenvalues) that completely characterizes the system.   Next we find that in the time domain, the impulse response is a way to "probe" a linear system to determine its behavior.  In other words both $H(s)$ and $h(t)$ are a very special responses that characterizes the system.  One comes from the complex exponential input $e^{st}$ and the other from the the application of a delta function.  How are these functions $H(s)$ and $h(t)$ related ? 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Relation to Complex Exponential}


Recall that we found that the complex exponential is an eigenfunction for our linear system 
\be
	\mathbf{L} [ e^{st} ] = H(s) e^{st} 
\ee
But that means that we can apply the convolution integral to the input $e^{st}$ and equate the two outputs
\be
	\mathbf{L} [ e^{st} ] = H(s) e^{st}  = \int_{-\infty}^{\infty} h(t-\tau) e^{s\tau} d\tau
\ee
A simple manipulation shows that:
\be
	H(s) = \int h(t-\tau) e^{s\tau} e^{-st} d\tau
\ee
\be
	H(s) = \int h(t-\tau) e^{-s(t- \tau)}  d\tau
\ee
\be
	H(s) = \int_{\infty}^{-\infty} h(x) e^{-sx} (-dx) = \int_{-\infty}^{\infty} h(x) e^{-sx} dx 
\ee
This equation shows us that $H(s)$ and $h(t)$ are indeed related in a very special way. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Laplace Transform}


 We derived a very important relation between the transfer function and the impulse response which is known as the Laplace Transform $\mathcal{L}$:
\be
	H(s) = \int_{-\infty}^{\infty} h(t) e^{-st} dt = \mathcal{L} \left\{ h(t) \right\}
\ee
This can be interpreted as transforming a time-domain function to the ``s-domain", which is the general complex frequency domain.  In this book we will not use Laplace Transforms explicitly, instead we will mostly use the transfer function in the frequency domain:  $H(j\omega)$, which is nothing but evaluating the transfer function $H(s)$ for $s=j\omega$.  

Even though we won't make extensive use of the Laplace Transform, we nevertheless spent some time to highlight these important concepts and relations because they show a deep connection between the frequency domain response $H(j\omega)$ and the time domain response.

 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{De-convolving the Convolution}


In fact, one of the most important relations is how convolution in time translates to the frequency domain.   Consider the (complicated) convolution operator and take the results into the Laplace domain
\be
	Y(s) = \mathcal{L} \left\{ x(t) * h(t) \right\} = \mathcal{L} \left\{  \int h(t-\tau) x(\tau) d\tau \right\} 
\ee
Use the definition of the Laplace Transform
\be
	= \int \left( \int h(t-\tau) x(\tau) d\tau \right) e^{-s t} dt
\ee
Notice that if we make a simple change of notation and call $y = t-\tau$ the new dummy variable, we have
\be
	= \int  \int h(\underbrace{t-\tau}_{y}) x(\tau)  e^{-s t}  d\tau dt =  \int  \int h(y) e^{-(y + \tau)} x(\tau) dy d\tau
\ee
Each integral is only a function of one variable, so we can separate them out:
\be
	= \left( \int h(y) e^{-sy} dy  \right) \left( \int x(\tau) e^{-s \tau} d\tau \right) 
\ee
 Or more simply, the convolution is a (simple) product in the Laplace domain:
\be
	Y(s) = H(s) X(s) 
\ee
This is a very important result.  Not only is it more efficient to work directly in the Laplace (or Frequency) domain when computing the response of a linear system, but it's also much easier to visualize what's going on.  A filter can be visualized very simply in the frequency domain. For an arbitrary input, if we compute it's Laplace (or Fourier, see below) Transform, we can find the output by simply multiplying the two and doing an inverse transform.  In many cases, we don't even need to do the inverse transform, which is not a very simple calculation to carry out.  We simply use the concept of frequency domain to better understand our system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsubsection{Laplace Transform Properties}

The properties of the Laplace Transform are summarized in Fig.~\ref{fig:laplace_table}.   Most are easily derived and are provided here just for reference.  We just proved the most important property related to the convolution.    

\begin{figure}[tb]
\begin{center}
\includegraphics[width=.9\columnwidth]{laplace_table.png}
\end{center}
\caption{Table of Laplace Transform properties. From  \href{https://en.Wikipedia.org/wiki/Laplace_transform}{Wikipedia} }\label{fig:laplace_table}
\end{figure}
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Laplace Transform Table}

The Table in Fig.~\ref{fig:laplace_xform} shows some common Laplace Transform "pairs", or the s-domain representation for some common functions.   This table is important because in practice we hardly ever evaluate the Laplace Transform integral, especially in the reverse direction.  Instead, the lookup table is used for common functions and special techniques, such as partial fraction expansions, can be used to put a rational function into standard form.  We showed earlier that most linear systems do in fact generate a rational function transfer function.  
 

\begin{figure}[tb]
\begin{center}
\includegraphics[width=\columnwidth]{laplace_transform.png}
\end{center}
\caption{Table of Laplace Transform pairs.  From \href{https://en.wikipedia.org/wiki/Laplace_transform}{Wikipedia} }\label{fig:laplace_xform}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Fourier Series and Transform}


In previous courses, you may have learned that you can represent a periodic function in time as a Fourier series
\be
	x(t) = \sum_{n = -\infty}^{\infty} c_n e^{j 2 \pi n f_0 t}
\ee
In this equation $f_0 = 1/T$ is the fundamental frequency of the function of period $T$.  For many practical signals of interest, we can go back and forth between the time or frequency domain, in the sense that the coefficients $c_n$ are a complete representation of the signal (invertable).  The FFT is an efficient way to compute the Fourier Series.   We know that the magnitude of the coefficients $c_n$ determine the amount of power concentrated around a certain frequency.  If a function varies gradually in time with a time scale say less that $T$, then coefficients above $n =  f_0 T $ will be smaller in magnitude.
 
The Fourier Transform is a natural extension of the Fourier Series into the continuous time domain and it works for non-periodic functions (it has a line spectrum for periodic functions)
\be
	X(\omega)  = \mathcal{F} \left\{ x(t) \right\} =  \int_{-\infty}^{\infty} e^{-j \omega t} x(t) dt
\ee
And the inverse transform is given by
\be
	x(t) =  \mathcal{F}^{-1} \left\{ x(t) \right\} = \int_{-\infty}^{\infty} e^{j \omega t} X(f) df
\ee
For most signals, this is a well defined transformation allowing us to think of a signal in either the time domain or frequency domain. Notice that the Fourier Transform is related to the Laplace Transform by
\be
	 \mathcal{F} \left\{ x(t) \right\}  = \left. X(s) \right|_{s = j\omega}  = X(j\omega)
\ee
This brings us full circle and shows that the frequency response $H(j\omega)$, the sinusoidal steady-state response, is in fact related to the Laplace Transform of the impulse response of our system.  While $H(s)$ is general, $H(j\omega)$, or the Fourier Transform of the impulse response, it a special case that applies to the periodic steady-state response.  




\subsection{Frequency Domain Interpretation}



\begin{figure}[tb]
\begin{center}
\includegraphics[width=\columnwidth]{sig_noise_freq.pdf}
\end{center}
\caption{The frequency domain representation of a signal plus noise (left).  The signal consists of tones (delta-functions in frequency domain) and noise (flat spectrum).  The signal is low-pass filtered, which is equivalent to multiplication by the frequency domain transfer function shown in the middle, resulting in the filtered signal and reduction in noise (right).  } \label{fig:freq_domain}
\end{figure}

If we transform the input function from the time domain to the frequency domain, then we can simply say that every frequency component in the input gets multiplied and phase shifted by the corresponding frequency domain component of the transfer function, as shown in Fig.~\ref{fig:freq_domain}.  This follows from the properties of convolution in time becoming multiplication in the frequency domain.  Understanding a filter is much much easier done in the frequency domain.  In this example we see a frequency domain representation of a signal with three tones, shown as Delta functions.  They are delta functions because the input signal consists of pure tones with a very long duration, and so all the energy is concentrated in these two frequency bands.  We also show that the signal is accompanied by "white noise", shown as a flat spectrum signal.  Although we have not proven this yet, it can be shown that noise is a wideband signal and can be modeled as a flat signal in the frequency domain.

If we low-pass filter the signal and noise, we expect that we will reduce the contribution of the noise.  In this example we suppose that the third tone is undesired, so we select the corner frequency of the filter just beyond the second tone, so that we do not attenuate the desired tones.  After filtering,   we would expect that the overall noise of the signal can be reduced without affecting the signals of interest.  This is shown by simply multiplying the frequency domain representation of the signal and noise with the filter response.  We see the noise contribution beyond the corner is reduced dramatically, and the third tone is also attenuated.
 

\subsubsection{Audio Example}

\begin{figure}[tb]
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=.45\columnwidth]{tonesintime.pdf} &
\includegraphics[width=.45\columnwidth]{noiseonly.pdf} \\
\end{tabular}
\end{center}
\caption{A audio signal consisting of a tone and the third harmonic (delayed by 1 second).  The signal will be added to the noisy signal shown on the right.} \label{fig:tones_time}
\end{figure}


\begin{figure}[tb]
\begin{center}
\includegraphics[width=.45\columnwidth]{signoise.pdf}
\end{center}
\caption{The noisy signal is shown.  The noise impacts both the amplitude and zero crossings of the signal.} \label{fig:sigplusnoise}
\end{figure}


In this example, the input signal consists of a pure tone (261.626 Hz) and its third harmonic, which is delayed with respect to the fundamental.  Also, the signal is corrupted by a lot of noise content.  The signals of interest are shown in Fig.~\ref{fig:tones_time}.   Now consider the signal + noise with noise power 10$\times$ lower (Signal-Noise Ratio (SNR) = 10 dB), as shown in Fig.~\ref{fig:sigplusnoise}.
	
Click the following links to listen to the signals:
\begin{itemize}

\item \href{http://rfic.eecs.berkeley.edu/~niknejad/photos/ee105/tones.mp3}{Tone + Third Harmonic, no noise}

\item  \href{http://rfic.eecs.berkeley.edu/~niknejad/photos/ee105/tones\_noise.mp3}{Signal + Noise}

\item \href{http://rfic.eecs.berkeley.edu/~niknejad/photos/ee105/noise.mp3}{Just the "white" noise}
\end{itemize}	

Now if we filter the signal + noise, we can reduce the noise without impacting the signal (since the signal is mostly at low frequency in this case):
	
\begin{itemize}
\item  \href{ http://rfic.eecs.berkeley.edu/~niknejad/photos/ee105/tones\_lpf.mp3}{Low-pass filtered signal + noise}
\end{itemize}
	
 This is used extensively in practice to improve the signal quality.  To improve the signal quality further, we build a band-pass filter to pick out just the tone of interest.  The first filter picks up the first tone, and eliminates most of the noise.

\begin{itemize}
\item \href{http://rfic.eecs.berkeley.edu/~niknejad/photos/ee105/tones\_bpf\_wC.mp3}{Band-pass filtered centered around first tone}
\end{itemize}
			
Now we apply a bandpass filter with a  wider passband, allowing both tones to pass.  We pickup both tones, but of course the downside is we hear more noise.

\begin{itemize}
\item  \href{http://rfic.eecs.berkeley.edu/~niknejad/photos/ee105/tones\_bpf.mp3}{Wideband filtered signal + noise} 
\end{itemize}

 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{But Most Systems are Non-Linear ...?}

As we have demonstrated throughout this chapter, linear system theory is powerful and very useful for design and analysis.  But you may object and say that most real systems are non-linear, so why is this stuff useful ?

When systems are non-linear, we often ``linearize" them about an operating point and use linear theory to understand the response to ``small" perturbations.  We'll be doing this extensively in this course. We'll find that the silicon transistor and diodes are non-linear, so we'll build ``small-signal" models that are linear.  This is useful as long as the signals of interest don't deviate too much from the operating point.  If they do, we need non-linear techniques.  Even in these situations,  we can gain useful insights from the small-signal approximation, and it's the starting point for design and analysis.
 


\section{Chapter Summary}

This chapter has taken us from the frequency domain AC style analysis to the time domain impulse response, and then all the way back full circle to the frequency domain transfer function.

 Frequency response allows us to completely characterize a system using the sinusoidal response.
 More generally, the frequency response is characterized by the complex exponential response $H(s)$.   The impulse response $h(t)$ completely characterizes a system in the time-domain, and we found a connection between the two approaches.   The transfer function $H(s)$ is the Laplace Transform of the impulse response $h(t)$.  The Fourier Transform is the Laplace transform evaluated on the imaginary axis, and AC circuit theory is how we calculation (or measure) the transfer function in practice.   Convolution in time simplifies to multiplication in the frequency domain (and vice versa).
 Many concepts are calculations are easier to carry out in the frequency domain (filters), and as an engineer your intuition for the frequency domain will increase throughout your career.

 
















